{
    "0": {
        "tokenized_sent": [
            "It",
            "is",
            "and",
            "does",
            "exactly",
            "what",
            "the",
            "description",
            "said",
            "it",
            "would",
            "be",
            "and",
            "would",
            "do",
            "."
        ],
        "neg_scope": []
    },
    "1": {
        "tokenized_sent": [
            "Could",
            "n't",
            "be",
            "happier",
            "with",
            "it",
            "."
        ],
        "neg_scope": [
            [
                0,
                2,
                3,
                4,
                5
            ]
        ]
    },
    "2": {
        "tokenized_sent": [
            "I",
            "was",
            "sketchy",
            "at",
            "first",
            "about",
            "these",
            "but",
            "once",
            "you",
            "wear",
            "them",
            "for",
            "a",
            "couple",
            "hours",
            "they",
            "break",
            "in",
            "they",
            "fit",
            "good",
            "on",
            "my",
            "board",
            "an",
            "have",
            "little",
            "wear",
            "from",
            "skating",
            "in",
            "them",
            "."
        ],
        "neg_scope": []
    },
    "3": {
        "tokenized_sent": [
            "They",
            "are",
            "a",
            "little",
            "heavy",
            "but",
            "wo",
            "n't",
            "get",
            "eaten",
            "up",
            "as",
            "bad",
            "by",
            "your",
            "grip",
            "tape",
            "like",
            "poser",
            "dc",
            "shoes",
            "."
        ],
        "neg_scope": [
            [
                6,
                8,
                9,
                10,
                11,
                12,
                13,
                14,
                15,
                16,
                17,
                18,
                19,
                20
            ]
        ]
    },
    "4": {
        "tokenized_sent": [
            "Solid",
            ",",
            "stable",
            "mount",
            "."
        ],
        "neg_scope": []
    },
    "5": {
        "tokenized_sent": [
            "Holds",
            "iPhone",
            "with",
            "phone",
            "protector",
            "well",
            "."
        ],
        "neg_scope": []
    },
    "6": {
        "tokenized_sent": [
            "I",
            "have",
            "not",
            "however",
            "used",
            "the",
            "dash",
            "mount",
            "part",
            "of",
            "this",
            "product",
            "-LRB-",
            "only",
            "windshield",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
                12,
                13,
                14
            ]
        ]
    },
    "7": {
        "tokenized_sent": [
            "I",
            "bought",
            "this",
            "pepper",
            "because",
            "I",
            "wanted",
            "a",
            "lot",
            "of",
            "cayenne",
            "powder",
            "I",
            "mean",
            "a",
            "lot",
            "."
        ],
        "neg_scope": []
    },
    "8": {
        "tokenized_sent": [
            "I",
            "drink",
            "shots",
            "of",
            "this",
            "powder",
            "daily",
            "and",
            "I",
            "do",
            "like",
            "it",
            "but",
            "I",
            "'m",
            "not",
            "sure",
            "if",
            "it",
            "'s",
            "just",
            "me",
            "but",
            "it",
            "does",
            "not",
            "seem",
            "to",
            "strong",
            "I",
            "sweat",
            "for",
            "a",
            "minute",
            "but",
            "I",
            "feel",
            "like",
            "it",
            "could",
            "be",
            "stronger",
            "I",
            "even",
            "touched",
            "my",
            "eyes",
            "to",
            "see",
            "if",
            "it",
            "would",
            "hurt",
            "still",
            "not",
            "feeling",
            "much",
            "pain",
            "."
        ],
        "neg_scope": [
            [
                13,
                14,
                16,
                17,
                18,
                19
            ],
            [
                23,
                24,
                26,
                27,
                28,
                29,
                30,
                31,
                32,
                33
            ],
            [
                55,
                56,
                57
            ]
        ]
    },
    "9": {
        "tokenized_sent": [
            "Im",
            "one",
            "in",
            "7",
            "billion",
            "so",
            "do",
            "forget",
            "my",
            "review",
            "since",
            "I",
            "'m",
            "more",
            "hardcore",
            "."
        ],
        "neg_scope": []
    },
    "10": {
        "tokenized_sent": [
            "Beautiful",
            "photos/film",
            "with",
            "wonderful",
            "music",
            "."
        ],
        "neg_scope": []
    },
    "11": {
        "tokenized_sent": [
            "Giriodi",
            "lets",
            "you",
            "know",
            "where",
            "you",
            "are",
            "with",
            "on",
            "screen",
            "notes",
            "."
        ],
        "neg_scope": []
    },
    "12": {
        "tokenized_sent": [
            "It",
            "helps",
            "to",
            "keep",
            "your",
            "heart",
            "in",
            "Colorado",
            "when",
            "you",
            "ca",
            "n't",
            "be",
            "there",
            "."
        ],
        "neg_scope": [
            [
                9,
                10,
                12,
                13
            ]
        ]
    },
    "13": {
        "tokenized_sent": [
            "My",
            "idea",
            "of",
            "Colorado",
            "is",
            "&#34;",
            "Mountains",
            "&#34;",
            "."
        ],
        "neg_scope": []
    },
    "14": {
        "tokenized_sent": [
            "Colorado",
            "Landscapes",
            "seemed",
            "to",
            "focus",
            "on",
            "the",
            "flatlands",
            "and",
            "foothills",
            "."
        ],
        "neg_scope": []
    },
    "15": {
        "tokenized_sent": [
            "It",
            "gave",
            "you",
            "no",
            "idea",
            "where",
            "you",
            "were",
            "-",
            "it",
            "could",
            "have",
            "used",
            "a",
            "narrator",
            "or",
            "notes",
            "on-screen",
            "."
        ],
        "neg_scope": [
            [
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
                12,
                13,
                14,
                15,
                16,
                17
            ]
        ]
    },
    "16": {
        "tokenized_sent": [
            "No",
            "matter",
            "what",
            "we",
            "did",
            "the",
            "bills",
            "just",
            "kept",
            "jamming",
            "in",
            "the",
            "machine",
            "."
        ],
        "neg_scope": [
            [
                1,
                2,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
                12
            ]
        ]
    },
    "17": {
        "tokenized_sent": [
            "The",
            "bill",
            "counts",
            "were",
            "not",
            "consistent",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                2,
                3,
                5
            ]
        ]
    },
    "18": {
        "tokenized_sent": [
            "We",
            "returned",
            "the",
            "product",
            "because",
            "it",
            "did",
            "not",
            "work",
            "correctly",
            "."
        ],
        "neg_scope": [
            [
                5,
                6,
                8,
                9
            ]
        ]
    },
    "19": {
        "tokenized_sent": [
            "i",
            "do",
            "not",
            "suggest",
            "buying",
            "this",
            "product",
            "i",
            "thought",
            "it",
            "seemed",
            "nice",
            "and",
            "everything",
            "but",
            "it",
            "did",
            "not",
            "work",
            "when",
            "i",
            "removed",
            "it",
            "from",
            "the",
            "packaging",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                3,
                4,
                5,
                6
            ],
            [
                15,
                16,
                18,
                19,
                20,
                21,
                22,
                23,
                24,
                25
            ]
        ]
    },
    "20": {
        "tokenized_sent": [
            "for",
            "this",
            "reason",
            "i",
            "do",
            "not",
            "trust",
            "the",
            "shipping",
            "methods",
            "that",
            "the",
            "seller",
            "uses",
            "."
        ],
        "neg_scope": [
            [
                3,
                4,
                6,
                7,
                8,
                9,
                10,
                11,
                12,
                13
            ]
        ]
    },
    "21": {
        "tokenized_sent": [
            "It",
            "is",
            "not",
            "a",
            "sticker",
            ",",
            "it",
            "is",
            "a",
            "Chritsmas",
            "story",
            "by",
            "itself",
            ",",
            "full",
            "of",
            "details",
            ",",
            "and",
            "cover",
            "a",
            "big",
            "space",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                3,
                4
            ]
        ]
    },
    "22": {
        "tokenized_sent": [
            "The",
            "condition",
            "of",
            "the",
            "book",
            "was",
            "exactly",
            "as",
            "described",
            "."
        ],
        "neg_scope": []
    },
    "23": {
        "tokenized_sent": [
            "There",
            "were",
            "minimal",
            "if",
            "any",
            "damages",
            ",",
            "and",
            "certainly",
            "nothing",
            "that",
            "would",
            "hinder",
            "the",
            "use",
            "of",
            "the",
            "book",
            "."
        ],
        "neg_scope": []
    },
    "24": {
        "tokenized_sent": [
            "Only",
            "negative",
            "."
        ],
        "neg_scope": []
    },
    "25": {
        "tokenized_sent": [
            "Thing",
            "I",
            "can",
            "say",
            "is",
            "when",
            "my",
            "fish",
            "swim",
            "near",
            "the",
            "sides",
            "of",
            "the",
            "tank",
            "at",
            "the",
            "top",
            "they",
            "are",
            "n't",
            "illuminated",
            "by",
            "the",
            "light",
            "from",
            "the",
            "center",
            "."
        ],
        "neg_scope": [
            [
                18,
                19,
                21,
                22,
                23,
                24,
                25,
                26,
                27
            ]
        ]
    },
    "26": {
        "tokenized_sent": [
            "The",
            "blue",
            "night",
            "light",
            "is",
            "beautiful",
            "."
        ],
        "neg_scope": []
    },
    "27": {
        "tokenized_sent": [
            "The",
            "led",
            "light",
            "gives",
            "a",
            "healthy",
            "glow",
            "during",
            "the",
            "day",
            ",",
            "and",
            "overall",
            "the",
            "Hood",
            "looks",
            "and",
            "works",
            "great",
            "."
        ],
        "neg_scope": []
    },
    "28": {
        "tokenized_sent": [
            "This",
            "book",
            "caught",
            "my",
            "eye",
            ",",
            "and",
            "when",
            "I",
            "read",
            "the",
            "sample",
            ",",
            "I",
            "was",
            "hooked",
            "."
        ],
        "neg_scope": []
    },
    "29": {
        "tokenized_sent": [
            "The",
            "final",
            "hook",
            "was",
            "one",
            "other",
            "reviewer",
            "explaining",
            "the",
            "unemotional",
            "aspect",
            "of",
            "the",
            "book",
            "'s",
            "dramatic",
            "swings",
            ",",
            "because",
            "of",
            "the",
            "method",
            "of",
            "writing",
            "employed",
            "."
        ],
        "neg_scope": []
    },
    "30": {
        "tokenized_sent": [
            "-LRB-",
            "I",
            "have",
            "often",
            "been",
            "accused",
            "of",
            "being",
            "unemotional",
            ",",
            "or",
            "anti-sentimental",
            "...",
            "which",
            "is",
            "not",
            "true",
            "."
        ],
        "neg_scope": [
            [
                13,
                14,
                16
            ]
        ]
    },
    "31": {
        "tokenized_sent": [
            "People",
            "show",
            "these",
            "depths",
            "of",
            "feeling",
            "in",
            "many",
            "different",
            "ways",
            "--",
            "not",
            "all",
            "of",
            "us",
            "are",
            "weepers",
            ",",
            "people",
            "...",
            "-RRB-",
            "And",
            "The",
            "Call",
            "delivered",
            "."
        ],
        "neg_scope": [
            [
                12,
                13,
                14,
                15,
                16
            ]
        ]
    },
    "32": {
        "tokenized_sent": [
            "The",
            "emotion",
            "is",
            "there",
            ",",
            "and",
            "it",
            "'s",
            "pretty",
            "raw",
            "at",
            "times",
            ",",
            "but",
            "it",
            "'s",
            "emoted",
            "in",
            "unexpected",
            "ways",
            "."
        ],
        "neg_scope": []
    },
    "33": {
        "tokenized_sent": [
            "The",
            "characters",
            "cracked",
            "me",
            "up",
            ",",
            "felt",
            "very",
            "real",
            ",",
            "and",
            "the",
            "story",
            "was",
            "believable",
            "and",
            "at",
            "times",
            "serious",
            ",",
            "then",
            "light",
            ",",
            "then",
            "laugh-out-loud",
            "funny",
            "."
        ],
        "neg_scope": []
    },
    "34": {
        "tokenized_sent": [
            "I",
            "really",
            "enjoyed",
            "the",
            "writing",
            "style",
            ",",
            "and",
            "although",
            "it",
            "did",
            "take",
            "a",
            "few",
            "pages",
            "of",
            "adjustment",
            ",",
            "I",
            "was",
            "very",
            "glad",
            "I",
            "stuck",
            "with",
            "it",
            "."
        ],
        "neg_scope": []
    },
    "35": {
        "tokenized_sent": [
            "In",
            "the",
            "same",
            "way",
            "that",
            "Cormac",
            "MacCarthy",
            "'s",
            "writing",
            "is",
            "at",
            "first",
            "difficult",
            "to",
            "take",
            "in",
            ",",
            "this",
            "was",
            "a",
            "little",
            "bit",
            "tough",
            "to",
            "&#34;",
            "get",
            "&#34;",
            "until",
            "I",
            "'d",
            "gotten",
            "through",
            "several",
            "pages",
            "."
        ],
        "neg_scope": []
    },
    "36": {
        "tokenized_sent": [
            "I",
            "have",
            "already",
            "recommended",
            "this",
            "to",
            "several",
            "people",
            "."
        ],
        "neg_scope": []
    },
    "37": {
        "tokenized_sent": [
            "Made",
            "me",
            "want",
            "to",
            "try",
            "writing",
            "again",
            "."
        ],
        "neg_scope": []
    },
    "38": {
        "tokenized_sent": [
            "I",
            "was",
            "already",
            "of",
            "this",
            "author",
            "'s",
            "opinion",
            ",",
            "and",
            "everything",
            "he",
            "said",
            "just",
            "cemented",
            "my",
            "position",
            "."
        ],
        "neg_scope": []
    },
    "39": {
        "tokenized_sent": [
            "He",
            "presents",
            "his",
            "history",
            ",",
            "how",
            "he",
            "came",
            "to",
            "be",
            "in",
            "the",
            "position",
            "he",
            "'s",
            "in",
            ",",
            "how",
            "his",
            "life",
            "experiences",
            "-LRB-",
            "especially",
            "outside",
            "of",
            "America",
            "-RRB-",
            "have",
            "molded",
            "his",
            "expectations",
            "and",
            "understanding",
            "of",
            "value",
            "and",
            "wealth",
            ",",
            "and",
            "he",
            "makes",
            "a",
            "great",
            "case",
            "with",
            "real",
            "examples",
            "of",
            "how",
            "simplification",
            "is",
            "the",
            "best",
            "way",
            "."
        ],
        "neg_scope": []
    },
    "40": {
        "tokenized_sent": [
            "You",
            "'ll",
            "be",
            "changed",
            "permanently",
            "by",
            "how",
            "many",
            "of",
            "your",
            "problems",
            "are",
            "&#34;",
            "first-world",
            "&#34;",
            "type",
            ",",
            "nothing",
            "more",
            "."
        ],
        "neg_scope": []
    },
    "41": {
        "tokenized_sent": [
            "Well",
            "worth",
            "the",
            "read",
            "--",
            "our",
            "culture",
            "could",
            "benefit",
            "greatly",
            "by",
            "more",
            "exposure",
            "to",
            ",",
            "and",
            "influence",
            "by",
            ",",
            "this",
            "book",
            "."
        ],
        "neg_scope": []
    },
    "42": {
        "tokenized_sent": [
            "most",
            "boys",
            "would",
            "eat",
            "this",
            "up",
            "."
        ],
        "neg_scope": []
    },
    "43": {
        "tokenized_sent": [
            "it",
            "has",
            "all",
            "the",
            "requisite",
            "characteristics",
            "of",
            "a",
            "movie",
            "for",
            "young",
            "boys",
            "."
        ],
        "neg_scope": []
    },
    "44": {
        "tokenized_sent": [
            "it",
            "was",
            "funny",
            ",",
            "it",
            "had",
            "some",
            "good",
            "fights",
            ",",
            "some",
            "out-smarting",
            "some",
            "bad",
            "guys",
            ",",
            "and",
            "explosions",
            "to",
            "boot",
            "."
        ],
        "neg_scope": []
    },
    "45": {
        "tokenized_sent": [
            "I",
            "did",
            "n't",
            "watch",
            "the",
            "whole",
            "thing",
            "with",
            "him",
            ",",
            "but",
            "picked",
            "up",
            "on",
            "some",
            "good",
            "lessons",
            "about",
            "being",
            "yourself",
            "and",
            "being",
            "true",
            "to",
            "friends",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                3,
                4,
                5,
                6,
                7,
                8
            ]
        ]
    },
    "46": {
        "tokenized_sent": [
            "not",
            "a",
            "morality",
            "lesson",
            ",",
            "but",
            "hey",
            "--",
            "better",
            "than",
            "lots",
            "of",
            "garbage",
            "out",
            "there",
            "."
        ],
        "neg_scope": [
            [
                1,
                2,
                3,
                9,
                10,
                11,
                12,
                13,
                14
            ]
        ]
    },
    "47": {
        "tokenized_sent": [
            "he",
            "really",
            "enjoyed",
            "it",
            "and",
            "went",
            "back",
            "to",
            "see",
            "some",
            "parts",
            "again/show",
            "me",
            "."
        ],
        "neg_scope": []
    },
    "48": {
        "tokenized_sent": [
            "he",
            "would",
            "have",
            "watched",
            "it",
            "again",
            "immediately",
            "if",
            "I",
            "had",
            "n't",
            "kiboshed",
            "the",
            "idea...-mom",
            "."
        ],
        "neg_scope": [
            [
                8,
                9,
                11,
                12,
                13
            ]
        ]
    },
    "49": {
        "tokenized_sent": [
            "Love",
            "this",
            "grill",
            "and",
            "the",
            "fact",
            "that",
            "the",
            "plates",
            "are",
            "removable",
            "to",
            "put",
            "in",
            "the",
            "dishwasher",
            "."
        ],
        "neg_scope": []
    },
    "50": {
        "tokenized_sent": [
            "Bun",
            "warmer",
            "does",
            "n't",
            "grill",
            "the",
            "buns",
            "but",
            "just",
            "warms",
            "them",
            "up",
            "but",
            "I",
            "did",
            "n't",
            "buy",
            "the",
            "unit",
            "for",
            "that",
            "anyways",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                2,
                4,
                5,
                6
            ],
            [
                13,
                14,
                16,
                17,
                18,
                19,
                20,
                21
            ]
        ]
    },
    "51": {
        "tokenized_sent": [
            "It",
            "'s",
            "also",
            "twice",
            "as",
            "big",
            "as",
            "my",
            "last",
            "George",
            "Foreman",
            "grill",
            "so",
            "I",
            "am",
            "a",
            "happy",
            "grilled",
            "."
        ],
        "neg_scope": []
    },
    "52": {
        "tokenized_sent": [
            "I",
            "buy",
            "these",
            "for",
            "my",
            "om",
            "every",
            "year",
            "at",
            "Christmas",
            "and",
            "she",
            "loves",
            "them",
            "."
        ],
        "neg_scope": []
    },
    "53": {
        "tokenized_sent": [
            "She",
            "would",
            "n't",
            "own",
            "any",
            "other",
            "brand",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                3,
                4,
                5,
                6
            ]
        ]
    },
    "54": {
        "tokenized_sent": [
            "I",
            "was",
            "skeptical",
            "when",
            "she",
            "first",
            "asked",
            "for",
            "them",
            "but",
            "they",
            "are",
            "high",
            "quality",
            "and",
            "durable",
            "."
        ],
        "neg_scope": []
    },
    "55": {
        "tokenized_sent": [
            "She",
            "wears",
            "them",
            "every",
            "day",
            "."
        ],
        "neg_scope": []
    },
    "56": {
        "tokenized_sent": [
            "Also",
            ",",
            "Amazon",
            "prices",
            "are",
            "better",
            "than",
            "anywhere",
            "on",
            "the",
            "web",
            "-",
            "have",
            "n't",
            "found",
            "them",
            "cheaper",
            "."
        ],
        "neg_scope": [
            [
                12,
                14,
                15,
                16
            ]
        ]
    },
    "57": {
        "tokenized_sent": [
            "I",
            "do",
            "n't",
            "ever",
            "read",
            "but",
            "this",
            "book",
            "changed",
            "me",
            "I",
            "love",
            "love",
            "this",
            "book",
            "I",
            "hated",
            "when",
            "I",
            "got",
            "to",
            "the",
            "last",
            "book",
            "I",
            "did",
            "n't",
            "want",
            "it",
            "to",
            "be",
            "over",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
                12,
                13,
                14
            ],
            [
                24,
                25,
                27,
                28,
                29,
                30,
                31
            ]
        ]
    },
    "58": {
        "tokenized_sent": [
            "This",
            "book",
            "was",
            "n't",
            "as",
            "good",
            "as",
            "50",
            "Shades",
            "oh",
            "Grey",
            "but",
            "it",
            "was",
            "a",
            "very",
            "good",
            "book",
            "I",
            "would",
            "tell",
            "someone",
            "to",
            "get",
            "this",
            "book",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                2,
                4,
                5,
                6,
                7,
                8,
                9,
                10
            ]
        ]
    },
    "59": {
        "tokenized_sent": [
            "The",
            "author",
            "has",
            "been",
            "there",
            "and",
            "done",
            "that",
            "when",
            "it",
            "comes",
            "to",
            "what",
            "he",
            "suggests",
            "."
        ],
        "neg_scope": []
    },
    "60": {
        "tokenized_sent": [
            "He",
            "was",
            "an",
            "Air",
            "Force",
            "SERE",
            "-LRB-",
            "Survival",
            "Evasion",
            "Resistance",
            "Escape",
            "-RRB-",
            "instructor",
            "for",
            "30",
            "years",
            "."
        ],
        "neg_scope": []
    },
    "61": {
        "tokenized_sent": [
            "His",
            "approach",
            "to",
            "the",
            "subject",
            "of",
            "survival",
            "is",
            "practical",
            "in",
            "that",
            "he",
            "does",
            "NOT",
            "approach",
            "the",
            "topic",
            "with",
            "primitive",
            "tool",
            "making",
            "or",
            "cutting",
            "trees",
            "down",
            "to",
            "make",
            "a",
            "shelter",
            "."
        ],
        "neg_scope": []
    },
    "62": {
        "tokenized_sent": [
            "His",
            "approach",
            "is",
            "quick",
            ",",
            "practical",
            "and",
            "effective",
            "."
        ],
        "neg_scope": []
    },
    "63": {
        "tokenized_sent": [
            "I",
            "highly",
            "recommend",
            "this",
            "book",
            "."
        ],
        "neg_scope": []
    },
    "64": {
        "tokenized_sent": [
            "I",
            "have",
            "purchased",
            "this",
            "product",
            "several",
            "times",
            "and",
            "have",
            "gotten",
            "the",
            "same",
            "poor",
            "results",
            ",",
            "the",
            "adhesive",
            "sucks",
            "."
        ],
        "neg_scope": []
    },
    "65": {
        "tokenized_sent": [
            "I",
            "have",
            "used",
            "it",
            "to",
            "hang",
            "many",
            "small",
            "objects",
            ",",
            "such",
            "as",
            "a",
            "tv",
            "remote",
            ",",
            "a",
            "small",
            "alarm",
            "clock",
            "or",
            "even",
            "to",
            "hide",
            "wires",
            "unde",
            "a",
            "cabinet",
            "."
        ],
        "neg_scope": []
    },
    "66": {
        "tokenized_sent": [
            "The",
            "adhesive",
            "just",
            "does",
            "not",
            "stick",
            "well",
            "to",
            "anything",
            ",",
            "except",
            "itself",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                2,
                3,
                5,
                6,
                7,
                8
            ]
        ]
    },
    "67": {
        "tokenized_sent": [
            "I",
            "have",
            "no",
            "complaints",
            "about",
            "the",
            "hook",
            "and",
            "loop",
            "side",
            ",",
            "but",
            "it",
            "is",
            "certainly",
            "not",
            "anything",
            "that",
            "should",
            "be",
            "called",
            "&#34;",
            "industrial",
            "strength",
            "&#34;",
            "if",
            "the",
            "adhesive",
            "cant",
            "even",
            "hold",
            "up",
            "a",
            "tv",
            "remote",
            ",",
            "3M",
            "please",
            "fix",
            "this",
            "problem",
            ",",
            "I",
            "know",
            "you",
            "have",
            "better",
            "permanent",
            "adhesive",
            "that",
            "can",
            "withstand",
            "a",
            "warm",
            "surface",
            ",",
            "but",
            "this",
            "product",
            "adhesive",
            "is",
            "a",
            "big",
            "FAIL",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                3,
                4,
                5,
                6
            ],
            [
                16,
                17,
                18,
                19,
                20,
                21,
                22,
                23,
                24,
                25,
                26,
                27,
                28,
                29,
                30
            ]
        ]
    },
    "68": {
        "tokenized_sent": [
            "Because",
            "I",
            "have",
            "not",
            "gotten",
            "the",
            "book",
            "yet",
            ",",
            "I",
            "may",
            "change",
            "my",
            "mind",
            "when",
            "I",
            "get",
            "it",
            "."
        ],
        "neg_scope": [
            [
                1,
                2,
                4,
                5,
                6,
                7
            ]
        ]
    },
    "69": {
        "tokenized_sent": [
            "I",
            "will",
            "let",
            "you",
            "know",
            "."
        ],
        "neg_scope": []
    },
    "70": {
        "tokenized_sent": [
            "This",
            "was",
            "a",
            "great",
            "book",
            ",",
            "easy",
            "read",
            ",",
            "laugh",
            "out",
            "loud",
            "funny",
            "."
        ],
        "neg_scope": []
    },
    "71": {
        "tokenized_sent": [
            "Cant",
            "wait",
            "to",
            "read",
            "Sugar",
            "Ticket",
            "."
        ],
        "neg_scope": []
    },
    "72": {
        "tokenized_sent": [
            "Highly",
            "recommend",
            "this",
            "book",
            "."
        ],
        "neg_scope": []
    },
    "73": {
        "tokenized_sent": [
            "Looking",
            "forward",
            "to",
            "reading",
            "all",
            "the",
            "other",
            "books",
            "by",
            "Susan",
            "Ricci",
            "."
        ],
        "neg_scope": []
    },
    "74": {
        "tokenized_sent": [
            "This",
            "Hammermill",
            "106125",
            "-",
            "Color",
            "Copy",
            "Paper",
            ",",
            "100",
            "Brightness",
            ",",
            "28",
            "lb",
            ",",
            "12",
            "x",
            "18",
            ",",
            "Photo",
            "...",
            "paper",
            "works",
            "great",
            "."
        ],
        "neg_scope": []
    },
    "75": {
        "tokenized_sent": [
            "does",
            "not",
            "curl",
            "like",
            "other",
            "brands",
            "."
        ],
        "neg_scope": [
            [
                0,
                2,
                3,
                4,
                5
            ]
        ]
    },
    "76": {
        "tokenized_sent": [
            "the",
            "hooks",
            "were",
            "not",
            "chipped",
            "shipping",
            "was",
            "really",
            "fast",
            "nothing",
            "was",
            "broken",
            "all",
            "hooks",
            "were",
            "in",
            "package",
            "as",
            "described",
            "with",
            "all",
            "the",
            "sizes",
            "A",
            "+",
            "+",
            "+",
            "+",
            "+",
            "thank",
            "you",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                2,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
                12,
                13,
                14,
                15,
                16
            ]
        ]
    },
    "77": {
        "tokenized_sent": [
            "My",
            "son",
            "loves",
            "these",
            "gloves",
            "!"
        ],
        "neg_scope": []
    },
    "78": {
        "tokenized_sent": [
            "They",
            "'re",
            "the",
            "only",
            "ones",
            "he",
            "has",
            "n't",
            "lost",
            "this",
            "winter",
            "!"
        ],
        "neg_scope": [
            [
                5,
                6,
                8,
                9,
                10
            ]
        ]
    },
    "79": {
        "tokenized_sent": [
            "Will",
            "be",
            "ordering",
            "them",
            "again",
            "next",
            "winter",
            "."
        ],
        "neg_scope": []
    },
    "80": {
        "tokenized_sent": [
            "Product",
            "is",
            "not",
            "long",
            "enough",
            "to",
            "secure",
            "to",
            "headrests",
            "in",
            "my",
            "2013",
            "dodge",
            "ram",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10,
                11,
                12,
                13
            ]
        ]
    },
    "81": {
        "tokenized_sent": [
            "Product",
            "seems",
            "like",
            "it",
            "is",
            "meant",
            "for",
            "small",
            "vehicles",
            "."
        ],
        "neg_scope": []
    },
    "82": {
        "tokenized_sent": [
            "I",
            "just",
            "received",
            "a",
            "set",
            "of",
            "Japanese",
            "Hiragana",
            "and",
            "Katakana",
            "flash",
            "cards",
            "by",
            "Glen",
            "McCabe",
            "as",
            "recently",
            "revised",
            "by",
            "Professor",
            "Emiko",
            "Konomi",
            "."
        ],
        "neg_scope": []
    },
    "83": {
        "tokenized_sent": [
            "I",
            "am",
            "really",
            "impressed",
            ",",
            "both",
            "by",
            "the",
            "low",
            "price",
            "and",
            "by",
            "the",
            "overall",
            "quality",
            "and",
            "value",
            "of",
            "the",
            "set",
            "."
        ],
        "neg_scope": []
    },
    "84": {
        "tokenized_sent": [
            "The",
            "real",
            "usefulness",
            "of",
            "a",
            "set",
            "like",
            "this",
            "is",
            "in",
            "the",
            "way",
            "it",
            "can",
            "facilitate",
            "the",
            "speed",
            "and",
            "accuracy",
            "of",
            "character",
            "recognition",
            "."
        ],
        "neg_scope": []
    },
    "85": {
        "tokenized_sent": [
            "I",
            "'m",
            "looking",
            "forward",
            "to",
            "using",
            "it",
            "this",
            "way",
            "."
        ],
        "neg_scope": []
    },
    "86": {
        "tokenized_sent": [
            "The",
            "cards",
            "are",
            "plastic",
            "coated",
            ",",
            "so",
            "they",
            "wo",
            "n't",
            "get",
            "``",
            "funky",
            "''",
            "like",
            "the",
            "ones",
            "I",
            "made",
            "for",
            "myself",
            "some",
            "time",
            "ago",
            "."
        ],
        "neg_scope": [
            [
                7,
                8,
                10,
                11,
                12,
                13,
                14,
                15,
                16,
                17,
                18,
                19,
                20,
                21,
                22,
                23
            ]
        ]
    },
    "87": {
        "tokenized_sent": [
            "Each",
            "card",
            "has",
            "a",
            "series",
            "of",
            "words",
            "written",
            "in",
            "kana",
            ",",
            "which",
            "aids",
            "in",
            "the",
            "reading",
            "of",
            "whole",
            "words",
            "."
        ],
        "neg_scope": []
    },
    "88": {
        "tokenized_sent": [
            "One",
            "of",
            "the",
            "most",
            "important",
            "features",
            "involves",
            "a",
            "high",
            "quality",
            "CD",
            "that",
            "comes",
            "with",
            "the",
            "cards",
            "."
        ],
        "neg_scope": []
    },
    "89": {
        "tokenized_sent": [
            "Using",
            "the",
            "CD",
            "in",
            "conjunction",
            "with",
            "the",
            "cards",
            "allows",
            "the",
            "learner",
            "to",
            "completely",
            "avoid",
            "using",
            "roomaji",
            ",",
            "and",
            "go",
            "directly",
            "from",
            "the",
            "Japanese",
            "character",
            "to",
            "the",
            "correct",
            "Japanese",
            "pronunciation",
            "."
        ],
        "neg_scope": []
    },
    "90": {
        "tokenized_sent": [
            "The",
            "200",
            "cards",
            "include",
            "the",
            "so-called",
            "``",
            "twisted",
            "sounds",
            "''",
            "as",
            "well",
            "as",
            "the",
            "``",
            "voiced",
            "''",
            "sounds",
            "."
        ],
        "neg_scope": []
    },
    "91": {
        "tokenized_sent": [
            "There",
            "are",
            "also",
            "wall",
            "charts",
            "of",
            "the",
            "characters",
            ",",
            "which",
            ",",
            "sad",
            "to",
            "say",
            ",",
            "I",
            "still",
            "find",
            "necessary",
            "on",
            "occasion",
            "."
        ],
        "neg_scope": []
    },
    "92": {
        "tokenized_sent": [
            "For",
            "the",
            "beginner",
            ",",
            "such",
            "charts",
            "would",
            "be",
            "of",
            "great",
            "value",
            "."
        ],
        "neg_scope": []
    },
    "93": {
        "tokenized_sent": [
            "I",
            "am",
            "currently",
            "teaching",
            "``",
            "Scientific",
            "Communication",
            "in",
            "English",
            "''",
            "at",
            "the",
            "Tokorozawa",
            "Campus",
            "of",
            "Waseda",
            "University",
            "."
        ],
        "neg_scope": []
    },
    "94": {
        "tokenized_sent": [
            "I",
            "am",
            "a",
            "colleague",
            "of",
            "Konomi-sensei",
            "at",
            "Portland",
            "State",
            "University",
            "and",
            "have",
            "also",
            "been",
            "a",
            "student",
            "in",
            "a",
            "Japanese",
            "class",
            "in",
            "which",
            "she",
            "was",
            "the",
            "instructor",
            "."
        ],
        "neg_scope": []
    },
    "95": {
        "tokenized_sent": [
            "She",
            "is",
            "truly",
            "a",
            "master",
            "teacher",
            ",",
            "and",
            "has",
            "used",
            "her",
            "considerable",
            "skills",
            "to",
            "produce",
            "a",
            "valuable",
            "and",
            "useful",
            "aid",
            "in",
            "the",
            "flash",
            "card",
            "set",
            "."
        ],
        "neg_scope": []
    },
    "96": {
        "tokenized_sent": [
            "The",
            "case",
            "feels",
            "a",
            "bit",
            "cheap",
            "but",
            "overall",
            "it",
            "'s",
            "a",
            "nice",
            "case",
            "."
        ],
        "neg_scope": []
    },
    "97": {
        "tokenized_sent": [
            "A",
            "but",
            "hard",
            "to",
            "use",
            "when",
            "a",
            "call",
            "comes",
            "in",
            "but",
            "if",
            "you",
            "usually",
            "use",
            "headsets",
            "it",
            "wo",
            "n't",
            "be",
            "a",
            "problem",
            "."
        ],
        "neg_scope": [
            [
                16,
                17,
                19,
                20,
                21
            ]
        ]
    },
    "98": {
        "tokenized_sent": [
            "A",
            "great",
            "little",
            "bit",
            "."
        ],
        "neg_scope": []
    },
    "99": {
        "tokenized_sent": [
            "I",
            "was",
            "having",
            "trouble",
            "finding",
            "a",
            "bit",
            "in",
            "this",
            "size",
            "and",
            "was",
            "glad",
            "to",
            "have",
            "found",
            "it",
            "."
        ],
        "neg_scope": []
    },
    "100": {
        "tokenized_sent": [
            "Not",
            "disappointed",
            "in",
            "the",
            "performance",
            "."
        ],
        "neg_scope": [
            [
                1,
                2,
                3,
                4
            ]
        ]
    },
    "101": {
        "tokenized_sent": [
            "Holds",
            "up",
            "well",
            "to",
            "heven",
            "harder",
            "woods",
            "."
        ],
        "neg_scope": []
    },
    "102": {
        "tokenized_sent": [
            "I",
            "'m",
            "sorry",
            "but",
            "this",
            "movie",
            "was",
            "horrible",
            "."
        ],
        "neg_scope": []
    },
    "103": {
        "tokenized_sent": [
            "i",
            "ca",
            "n't",
            "believe",
            "i",
            "actually",
            "sat",
            "through",
            "the",
            "whole",
            "thing",
            "."
        ],
        "neg_scope": [
            [
                0,
                1,
                3,
                4,
                5,
                6,
                7,
                8,
                9,
                10
            ]
        ]
    },
    "104": {
        "tokenized_sent": [
            "The",
            "acting",
            "was",
            "bad",
            ",",
            "the",
            "plot",
            "sucked",
            ",",
            "and",
            "nobody",
            "actually",
            "knew",
            "how",
            "to",
            "play",
            "the",
            "guitar",
            "."
        ],
        "neg_scope": []
    },
    "105": {
        "tokenized_sent": [
            "I",
            "'ll",
            "give",
            "it",
            "this",
            ",",
            "some",
            "of",
            "the",
            "humor",
            "was",
            "good",
            ",",
            "but",
            "that",
            "was",
            "the",
            "only",
            "good",
            "point",
            "."
        ],
        "neg_scope": []
    },
    "106": {
        "tokenized_sent": [
            "The",
            "music",
            "may",
            "be",
            "considered",
            "good",
            "by",
            "some",
            "but",
            "to",
            "me",
            "the",
            "songs",
            "grated",
            "on",
            "my",
            "nerves",
            "and",
            "all",
            "the",
            "background",
            "music",
            "was",
            "cheap",
            "and",
            "overused",
            "."
        ],
        "neg_scope": []
    },
    "107": {
        "tokenized_sent": [
            "This",
            "might",
            "be",
            "acceptable",
            "for",
            "kids",
            ",",
            "but",
            "it",
            "certainly",
            "is",
            "not",
            "for",
            "me",
            "."
        ],
        "neg_scope": [
            [
                10,
                12,
                13
            ]
        ]
    },
    "108": {
        "tokenized_sent": [
            "Oh",
            ",",
            "and",
            "cinderella",
            "is",
            "saddened",
            "by",
            "this",
            "lack",
            "of",
            "respect",
            "to",
            "her",
            "name",
            "."
        ],
        "neg_scope": []
    }
}